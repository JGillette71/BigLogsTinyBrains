---
marp: true
title: SLLIM - System Log Local Intelligent Model
theme: default
paginate: true
<!-- Open this presentation with MARP installed -->
<!-- marketplace.visualstudio.com/items?itemName=marp-team.marp-vscode -->
---

# SLLIM: System Log Local Intelligent Model

**Authors**: Carlos Cruzportillo, Nassos Galiopoulos, Jason Gillette

**Affiliation**: University of Texas at San Antonio  

**Date**: October 7th, 2024

---

# Introduction

In today’s digital age, the widespread adoption of internet-connected devices—ranging from IoT devices to mobile phones—has created an unprecedented volume of data. Enterprises, relying on complex systems, face a growing challenge in monitoring, managing, and securing system-generated logs. In securing those environments, it is crucial to have a robust system log analysis tool that can quickly and accurately identify potential threats and anomalies. Traditional methods rely heavily on static pattern recognition techniques, which often fall short when faced with the dynamic nature of modern log data. This paper introduces a novel approach leveraging lightweight LLMs for real-time system log analysis, with a focus on balancing performance and computational efficiency.

---

# Problem Statement

The increasing volume of system logs generated by interconnected devices and enterprise systems creates a challenge for IT professionals in efficiently detecting threats and diagnosing issues, necessitating the development of lightweight, intelligent tools for real-time log analysis and query.

---

# Research Questions

1. How well can lightweight LLMs detect system issues and security threats from system logs?
2. How effectively can lightweight LLMs perform question answering compared to larger, more resource-intensive models?

---

# Specific Objectives

1. Fine-tune at least two lightweight LLMs for comparative analysis.
2. Evaluate question answering performance of lightweight LLMs versus resource-intensive models in the cybersecurity domain.

---

# Literature Review

1. **[Paper 1]**: Summary and relevance
2. **[Paper 2]**: Summary and relevance
3. **[Paper 3]**: Summary and relevance
4. **[Paper 4]**: Summary and relevance
5. **[Paper 5]**: Summary and relevance

---

# Paper 1

The paper "On-Device Language Models: A Comprehensive Review" provides a detailed exploration of strategies for deploying LLMs on edge devices, which is highly relevant to our objective of deploying modular, small-scale LLMs for offline forensic analysis. The paper covers various methods to address the challenges of limited computational power, reduced memory, and energy constraints on edge devices.

Key techniques discussed include:

Quantization: Quantization transforms high-precision (floating-point) weights and activations into lower bit-widths (e.g., integers). This reduces the model size and computational demands, enabling faster inference and decreased memory consumption, which is essential for efficient deployment on resource-constrained devices. Quantization-aware training (QAT) and post-training quantization (PTQ) are both used, with QAT integrating quantization directly into training to maintain accuracy, while PTQ is applied after training for a faster and less resource-intensive process.

Pruning: Pruning selectively removes parameters to reduce complexity and improve computational efficiency. Structured Pruning removes entire layers or channels, beneficial for hardware optimization, while Unstructured Pruning removes individual weights for higher compression rates. Contextual Pruning targets parameters based on their operational relevance, which helps balance efficiency and functionality, making it practical for on-device scenarios where resource optimization is critical.

Knowledge Distillation: Knowledge distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student). Black-box Knowledge Distillation uses only the outputs of the teacher model, while White-box Knowledge Distillation involves learning from the internal states and workings of the teacher, resulting in more accurate models. This technique helps reduce model size while retaining core capabilities, supporting our objective to make LLMs feasible for on-device use without compromising performance.

Low-Rank Factorization: This technique decomposes matrices into smaller components, reducing computational complexity and memory usage while preserving accuracy. Low-Rank Compensation (LoRC) combines low-rank factorization with quantization to further enhance efficiency, making it suitable for edge deployments.

The paper also discusses hardware acceleration strategies, including the use of GPUs, NPUs, and FPGAs to enhance the efficiency of LLMs on edge devices. These accelerators provide specialized support for machine learning workloads, allowing models to run effectively despite hardware constraints. Additionally, collaborative and hierarchical model approaches are highlighted, where the computational load is distributed across multiple devices or servers to optimize resource utilization.

Evaluation Methods: The evaluation of on-device LLMs uses several metrics:

Latency: Measured by Time-to-First-Token (TTFT), indicating the delay from user input to the first model response. Low latency is crucial for real-time applications.

Inference Speed: The speed at which the LLM predicts the next token, determining the smoothness of user interactions.

Memory Usage: The amount of RAM/VRAM consumed during inference, a critical factor for resource-limited devices.

Energy Consumption: Evaluates power usage during inference, which impacts battery life and device usability.

These methods help determine the feasibility of deploying LLMs in constrained environments, directly aligning with our objective to evaluate lightweight LLM performance on edge devices for forensic analysis. The detailed exploration of quantization, pruning, knowledge distillation, and hardware acceleration offers insights into optimizing LLMs to meet our research goals.

---

# Paper 2


The paper "Mobile Edge Intelligence for Large Language Models: A Contemporary Survey" provides practical techniques for deploying lightweight LLMs on edge devices, particularly for real-time system log analysis. Mobile Edge Intelligence (MEI) uses edge networks with moderate computational resources to deploy LLMs closer to users, balancing privacy, latency, and computational load. This approach is crucial for developing a modular LLM architecture for effective on-device analysis.

The paper highlights several key technical strategies that align with our research objectives:

Quantization: Quantization reduces the precision of model parameters (e.g., converting to INT4), decreasing computational and storage requirements. This makes LLMs feasible for deployment on resource-constrained devices, helping achieve our goal of fine-tuning lightweight LLMs for efficient edge deployment while maintaining performance in detecting system issues and security threats.

Pruning: Pruning, both structured and unstructured, removes redundant parameters to reduce model size and computational load. This is critical for ensuring that lightweight LLMs perform well in environments with limited resources, supporting our objective of evaluating these models compared to larger, more resource-intensive counterparts.

Knowledge Distillation: Knowledge distillation allows a large model to transfer its knowledge to a smaller model, retaining core functionality while being more efficient for edge deployment. This helps address our research question on how effectively lightweight LLMs can perform tasks such as question answering compared to larger models.

The concept of modular AI systems in the MEI framework involves distributing tasks across smaller models deployed on edge nodes, improving efficiency by balancing computational load. This supports our goal of developing a modular, lightweight LLM architecture capable of handling real-time data analysis on edge devices.

Deployment strategies like containerization and split learning are also relevant. Containerization provides a portable and consistent environment for deploying LLMs, addressing dependency management challenges. Split learning divides the model between the device and an edge server, allocating resource-intensive computations to the server and keeping lighter tasks on the device. These strategies are key to evaluating the feasibility of lightweight LLMs for offline forensic analysis and comparing their performance to larger models in the cybersecurity domain.

---

# Paper 3

The paper "Mobile Evaluation of Language Transformers" evaluates the feasibility of deploying LLMs on mobile and edge devices using their own benchmarking infrastructure called MELT. This study is particularly relevant for understanding the constraints and performance of lightweight LLMs on resource-constrained environments, which aligns directly with our objective of deploying modular, small-scale LLMs for offline forensic analysis.

The paper uses MELT to benchmark popular instruction fine-tuned LLMs across various devices, including Android, iOS, and Nvidia Jetson. Key evaluation metrics include:

Performance Throughput: This measures the number of processed inputs per unit of time, which provides an indication of how well the LLM can handle real-time data. Higher throughput is critical for ensuring efficient log analysis without lag.

Energy Consumption: This evaluates the power usage of the LLM during inference. Lower energy consumption is essential for mobile and edge devices to prolong battery life and maintain device usability during operations.

Accuracy Degradation Due to Quantization: This assesses the drop in model accuracy after applying quantization techniques. Quantization reduces memory and computational requirements, but can lead to reduced accuracy. Understanding this trade-off is key to optimizing model deployment on edge devices where resources are limited.

The use of quantization reduces the memory requirements but leads to some accuracy loss, which is important for our goal of developing efficient LLMs for edge deployment. The evaluation also highlights the impact of device capabilities on LLM performance, emphasizing the memory-bound nature of inference on mobile devices, which helps us understand the feasibility of running these models in constrained environments.

The paper's methodology involves model quantization and the use of automated benchmarking tools to evaluate LLMs' performance on different hardware platforms. Quantization (e.g., converting to lower-bit representations) significantly improves deployment feasibility on edge devices, supporting our goal of deploying efficient LLMs for real-time system log analysis. The study also shows that continuous LLM execution impacts energy consumption and device responsiveness, underlining the importance of optimizing runtime and deployment strategy to balance performance and energy usage, which directly relates to our research questions on efficiency and practicality of lightweight LLMs for on-device tasks.

---

# Methodology: Lightweight LLMs

Lightweight models perform tasks such as question answering on system logs while minimizing resource consumption. Techniques like **model compression**, **knowledge distillation**, **pruning**, and **efficient layer design** ensure that these models run efficiently in resource-constrained environments.

---

# Methodology: Fine-Tuning Process

Fine-tuning involves embedding system log data into context, pairing it with questions, and generating prompts based on the ICQ (Instruction, Context, Question) pattern. This process allows the model to learn how to generate accurate responses to system log queries.

![Data Flow for Training](../assets/diagram-training-dataflow.png)

---

# Evaluation Criteria

We will use **BERTScore** for semantic similarity evaluation. It computes Precision, Recall, and F1 scores to evaluate how closely generated answers match the expected answers based on contextual embeddings, providing a robust metric for tasks where meaning is more important than exact word matching.

![Data Flow for Eval](../assets/diagram-evaluation-dataflow.png)

---

# Datasets

1. **Dataset 1**: [Placeholder]
2. **Dataset 2**: [Placeholder]

---

# Expected Outcomes

We hypothesize that lightweight LLMs will deliver faster inference times and consume fewer computational resources while maintaining competitive accuracy in domain-specific tasks like system log analysis, making them more efficient than larger models in this context.

---

# Project Timeline

| Milestone               | Deadline     |
|-------------------------|--------------|
| Proposal Submission      | 20241007     |
| Dataset Collection       | 20241014     |
| Model Training           | TBD          |
| Evaluation and Testing   | TBD          |
| Final Report Submission  | 20241202     |

---

# Challenges and Risks

1. **Data Quality**: Ensuring question-answer pairs are aligned with log data context in each prompt.
2. **Data Preparation**: Chunking or retrieving relevant log subsets as context without losing essential information.
3. **Model Overfitting**: Avoiding overfitting to domain-specific logs, which may hinder generalization to unseen questions.

---

# Mitigation Strategies

1. **Data Quality**: Implement strict human-in-the-loop validation to cross-check question-answer pairs against log data.
2. **Data Preparation**: Develop an automated log chunking and retrieval pipeline using retrieval tools to ensure full context is covered.
3. **Model Overfitting**: Use techniques like cross-validation, regularization, and dropout, and incorporate diverse training examples to maintain generalization.

---

# References

1. [Placeholder for Reference 1]
2. [Placeholder for Reference 2]
3. [Placeholder for Reference 3]
4. [Placeholder for Reference 4]
5. [Placeholder for Reference 5]
