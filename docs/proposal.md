# Project Proposal: SLLIM - System Log Local Intelligent Model

**Authors**: Carlos Cruzportillo, Nassos Galiopoulos, Jason Gillette

**Affiliation**: University of Texas at San Antonio Department of Information Technology and Cyber Security

**Date**: October 7th, 2024

---

## Introduction

In today’s digital age, the widespread adoption of internet connected devices ranging from Internet of Things (IoT) devices to mobile phones—has created an unprecedented volume of data. Enterprises are no exception, as their reliance on complex, distributed systems has led to an ever-growing digital footprint. With the increasing use of these systems comes the challenge of monitoring, managing, and securing vast amounts of system-generated data, particularly system logs. These logs provide crucial insights into the health, performance, and security of an organization’s IT infrastructure.

However, the sheer volume of logs generated across networks, applications, and devices presents a “big data” challenge for IT professionals. Identifying potential security threats, diagnosing system issues, and ensuring compliance in real-time has become increasingly difficult due to the overwhelming amount of log data. This complexity demands advanced tools that can intelligently process, analyze, and query logs efficiently and cost-effectively. Moreover, the tools must be lightweight and adaptable, ensuring they can be deployed across a range of environments without introducing significant resource overhead or latency.

**Problem Statement: The increasing volume of system logs generated by interconnected devices and enterprise systems create a challenge for IT professionals in efficiently detecting threats and diagnosing issues, necessitating the development of lightweight, intelligent tools for real-time log analysis and query.**

The proliferation of interconnected devices and enterprise systems has exacerbated the challenge of managing and securing IT infrastructure. As logs from these systems increase in both volume and complexity, IT professionals are faced with a significant data management problem, particularly in identifying security threats and diagnosing system issues in real time. Traditional methods of querying and analyzing logs are often time-consuming and resource-intensive, delaying critical responses to potential threats.

There is a clear market demand for intelligent, efficient, and lightweight tools that can expedite the analysis of system logs. These tools must not only provide meaningful insights but also do so without adding excessive operational overhead. To address this need, we propose developing a lightweight large language model (LLM) specifically trained for question answering on system logs. This solution will empower IT professionals to quickly triage issues, detect security threats, and respond in a timely manner, all while maintaining minimal resource consumption and ensuring ease of deployment across various infrastructures.

---

## Research Questions

  1. How well can a lightweight large language models (LLMs) detect system issues and security threats from system logs?
  2. How effectively can lightweight large language models perform question answering tasks compared to larger, more resource-intensive models?

  ---

## Specific Objectives

  1. Fine-tune at least (2) lightweight large language models for comparative analysis.
  2. Evaluate question answering performance of lightweight large language models against more popular and resource intensive large language models in the Cyber Security domain.

---

## Literature Review

  1. **Paper 1**: [Brief summary and relevance]
  2. **Paper 2**: [Brief summary and relevance]
  3. **Paper 3**: [Brief summary and relevance]
  4. **Paper 4**: [Brief summary and relevance]
  5. **Paper 5**: [Brief summary and relevance]

---

## Methodology

- **Data Collection Methods**: [**TODO**: Describe use of publicly available data sources.]
- **Lightweight LLMs**:  Lightweight models are designed to perform complex LLM tasks, such as question answering on system logs, while minimizing resource consumption. These models are optimized to run efficiently in environments with limited computational power or memory, such as a local machine. Multiple techniques are used to optimize lightweight models. Techniques like model compression reduce the number of parameters and layers in the model, making it smaller and faster. Knowledge distillation allows the lightweight model to learn from a larger, pre-trained model, retaining high performance while being compact. Additionally, methods like pruning can remove redundant parameters to streamline inference. Finally, efficient layer designs further optimizes model architecture for faster inference. This research project aims to test fine-tuned versions of several lightweight that may contain any number of these techniques. By leveraging a lightweight model
- **Model fine-tuning**: In the fine-tuning process, system log data is embedded into context, and iteratively paired with a training set questions that represent complete inquiries. These inputs, along with explicit instructions regarding the task, are used to create prompts for the model. This prompt construction follows the ICQ (Instruction, Context, Question) pattern of prompt engineering. The model is then fine-tuned by processing these prompts and comparing its output to the expected answers. Over iterative examples, the model learns to generate accurate and efficient responses to system log queries, helping IT professionals detect issues and threats more quickly.

```mermaid
graph TD;
    subgraph Training Data Flow
        A[Question Set] --> B[Question]
        C[Log Data] --> D[Context]
        B --> F[Prompt]
        D --> F
        E[Instruction] --> F
        F --> G[Model]
        H[Answer Set] --> G
    end
```

- **Evaluation Criteria**: For a comparative analysis of lightweight models versus a large language model, we will use a semantic similarity metric via a tool called BERTScore. BERTScore is a semantic evaluation mechanism used to assess the quality of generated text by comparing it to ground truth answer at the token level using contextual embeddings from pre-trained BERT models. Other popular question answering evaluations are BLEU or ROUGE. However, these rely on exact n-gram matches which my not capture contextual performance. BERTScore captures the meaning of the text by computing embeddings for both the generated and reference answers. It then measures the similarity between these embeddings, making it particularly useful for tasks where semantic similarity is more important than exact word matching, i.e., question answering on system logs. BERTScore computes three key metrics: Precision, Recall, and F1 Score. Precision measures how much of the generated text is relevant to the reference, while Recall evaluates how much of the reference text is captured by the generated output. The F1 Score, which balances Precision and Recall, is the most commonly used for overall evaluation and provides a robust measure of the semantic similarity between generated answers and the expected answers. By averaging these scores across all sentences in the dataset, an overall BERTScore is calculated.

```mermaid
graph TD;
    subgraph Evaluation Data Flow
        C[Question Set] --> D[Question];
        A[Log Data] --> B[Context];
        D --> E[Prompt];
        B --> E;
        F[Instruction] --> E;
        E[Prompt] --> G[Model];
        G --> H[Evaluation];
        I[Answer Set] --> H;
    end
```

---

## Datasets
- **Training and Testing Datasets**:
  - Dataset 1: [Description]
  - Dataset 2: [Description]
- **Justification for Dataset Choice**: [Explain why these datasets are relevant]

---

## Expected Outcomes

In our comparative analysis of lightweight LLMs to larger models for the question answering task, we hypothesize that lightweight LLMs will deliver **faster inference** times and **consume fewer computational resources** while **maintaining competitive accuracy** for domain-specific tasks like system log analysis. While larger models possess vast parameter counts and a broader general knowledge base, our fine-tuned lightweight models will be optimized for question answering on system logs—requires; a more focused narrow-domain knowledge rather than vast, general knowledge.

---

## Project Timeline
| Milestone        | Deadline         |
|------------------|------------------|
| Proposal Submission | [20241007]          |
| Dataset Collection  | [20241014]          |
| Model Training      | [TBD]          |
| Evaluation and Testing | [TBD]      |
| Final Report Submission | [20241202]     |

---

## Challenges and Risks
- **Potential Challenges**:
  1. Data Quality: Ensuring that the generated question-answer pairs are aligned with the log data used as context in each prompt will be challenging. If the question-answer pairs do not accurately reflect the information in the logs, it could negatively impact the model’s ability to train.
  2. Data Preparation: Managing the complexity of long and unstructured system logs requires careful preprocessing. Chunking large logs or using other data retrieval methods to extract the most relevant sections as prompt context will be critical. The challenge is ensuring that these extracted subsets are both representative and concise enough for efficient model input without losing essential information.
  3. Model Overfitting: There is a risk of the lightweight model overfitting to the domain-specific system logs during fine-tuning, which could cause it to lose its ability to generalize to new, unseen questions.
- **Mitigation Strategies**:
  1. Data Quality: To ensure alignment, implement strict human-in-the-loop validation that cross-check question-answer pairs against the log data.
  2. Data Preparation: Develop an automated log chunking and retrieval pipeline that leverages existing retrival tools to extract the most relevant sections of logs, while ensuring coverage of the full context.
  3. Model Overfitting: To mitigate overfitting, use techniques like cross-validation, regularization, and dropout, and incorporate diverse training examples with a mix of both seen and unseen system logs to maintain generalization.

---

## References
1. [Reference for Paper 1]
2. [Reference for Paper 2]
3. [Reference for Paper 3]
4. [Reference for Paper 4]
5. [Reference for Paper 5]

## TODO

- [TinyAgent: Function Calling at the Edge]()
- [On Device Language Models: A Comprehensive Review]()
- [BERT-Log: Anomaly Detection for System Logs Based on Pre-trained Language Model](https://www.tandfonline.com/doi/full/10.1080/08839514.2022.2145642#abstract)
- [CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain](https://dl.acm.org/doi/full/10.1145/3652594)
- [LogQA: Question Answering in Unstructured Logs](https://arxiv.org/pdf/2303.11715)
- [BERT Score](https://huggingface.co/spaces/evaluate-metric/bertscore)
- [Benchmarking Large Language Models for Log Analysis, Security, and Interpretation](https://  link-springer-com.libweb.lib.utsa.edu/article/10.1007/s10922-024-09831-x)